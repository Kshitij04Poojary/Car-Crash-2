{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8829388,"sourceType":"datasetVersion","datasetId":5312326},{"sourceId":8831197,"sourceType":"datasetVersion","datasetId":5313720},{"sourceId":372408,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":308174,"modelId":328607}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# /kaggle/input/car-crash-dataset-ccd/CrashBest\n# /kaggle/input/car-crash-dataset-ccd/CrashBest/C_000001_01.jpg\n# /kaggle/input/crash-1500/Crash-1500/000001.mp4\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-16T13:50:37.766324Z","iopub.status.idle":"2024-07-16T13:50:37.767145Z","shell.execute_reply.started":"2024-07-16T13:50:37.766865Z","shell.execute_reply":"2024-07-16T13:50:37.766885Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install ffmpeg","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-04-21T10:28:58.434916Z","iopub.execute_input":"2025-04-21T10:28:58.435262Z","iopub.status.idle":"2025-04-21T10:29:06.633305Z","shell.execute_reply.started":"2025-04-21T10:28:58.435227Z","shell.execute_reply":"2025-04-21T10:29:06.632478Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\nHit:2 http://archive.ubuntu.com/ubuntu focal InRelease                         \nGet:3 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]\nGet:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]        \nGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [2502 kB]\nGet:6 https://packages.cloud.google.com/apt gcsfuse-focal InRelease [1227 B]   \nGet:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [128 kB]      \nGet:8 https://packages.cloud.google.com/apt cloud-sdk InRelease [1618 B]      \nGet:9 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [33.2 kB]\nGet:10 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [4548 kB]\nGet:11 https://packages.cloud.google.com/apt google-fast-socket InRelease [1071 B]\nGet:12 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [4780 kB]\nGet:13 https://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [36.9 kB]\nGet:14 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1306 kB]\nGet:15 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [4320 kB]\nGet:16 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [36.8 kB]\nGet:17 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1605 kB]\nGet:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4829 kB]\nGet:19 https://packages.cloud.google.com/apt cloud-sdk/main all Packages [1683 kB]\nGet:20 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [3727 kB]\nGet:21 https://packages.cloud.google.com/apt google-fast-socket/main amd64 Packages [15.4 kB]\nFetched 29.8 MB in 2s (13.2 MB/s)                           \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n0 upgraded, 0 newly installed, 0 to remove and 147 not upgraded.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\ncsv_file_path = '/kaggle/input/preprocessed-df/preprocessed_df (1).csv'\nannotations = pd.read_csv(csv_file_path)\n\nclass_counts = annotations['Severity of the Crash'].value_counts()\nprint(class_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:19:00.384669Z","iopub.execute_input":"2025-05-20T11:19:00.385582Z","iopub.status.idle":"2025-05-20T11:19:00.809583Z","shell.execute_reply.started":"2025-05-20T11:19:00.385538Z","shell.execute_reply":"2025-05-20T11:19:00.808553Z"}},"outputs":[{"name":"stdout","text":"Severity of the Crash\nModerate    622\nMinor       348\nSevere      244\nmoderate     70\nsevere       57\nminor        31\nfatal        25\nFatal        18\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"annotations['Severity of the Crash'] = annotations['Severity of the Crash'].str.lower()\nclass_counts = annotations['Severity of the Crash'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2025-05-20T11:19:02.924388Z","iopub.execute_input":"2025-05-20T11:19:02.924753Z","iopub.status.idle":"2025-05-20T11:19:02.933354Z","shell.execute_reply.started":"2025-05-20T11:19:02.924725Z","shell.execute_reply":"2025-05-20T11:19:02.932267Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Severity of the Crash\nmoderate    692\nminor       379\nsevere      301\nfatal        43\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nvideo_dir = '/kaggle/input/crash-1500/Crash-1500'\nframes_dir = '/kaggle/working/frames' \n\nfor idx, row in annotations.iterrows():\n    video_number = row['Video Number']\n    video_path = os.path.join(video_dir, f'{video_number:06}.mp4')\n    output_dir = os.path.join(frames_dir, f'{video_number:06}')\n    os.makedirs(output_dir, exist_ok=True)\n\n    ffmpeg_command = f'ffmpeg -ss 00:00:01 -i \"{video_path}\" -vf fps=5 \"{output_dir}/frame%04d.png\"'\n#     print(f\"Running FFmpeg command: {ffmpeg_command}\")\n    os.system(ffmpeg_command)\n    \n    extracted_frames = os.listdir(output_dir)\n    print(f\"Extracted {len(extracted_frames)} frames for video {video_number}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nframes_dir = '/kaggle/working/frames'\nvideo_dirs = next(os.walk(frames_dir))[1]\nnum_videos = len(video_dirs)\n\nprint(f\"Number of videos with extracted frames: {num_videos}\")","metadata":{"execution":{"iopub.status.busy":"2025-05-20T11:19:05.549430Z","iopub.execute_input":"2025-05-20T11:19:05.550172Z","iopub.status.idle":"2025-05-20T11:19:05.556576Z","shell.execute_reply.started":"2025-05-20T11:19:05.550143Z","shell.execute_reply":"2025-05-20T11:19:05.555646Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Number of videos with extracted frames: 1415\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport glob\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nfrom PIL import Image\n\nclass VideoDataset(Dataset):\n    def __init__(self, annotations_df, root_dir, transform=None, num_frames=20):\n        self.annotations = annotations_df\n        self.annotations['Severity of the Crash'] = self.annotations['Severity of the Crash'].str.lower()\n        self.root_dir = root_dir\n        self.transform = transform\n        self.label_map = {'minor': 0, 'moderate': 1, 'severe': 2,'fatal': 3}\n        self.num_frames = num_frames\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        video_number = self.annotations.iloc[idx]['Video Number']\n        label = self.label_map[self.annotations.iloc[idx]['Severity of the Crash']]\n        frame_dir = os.path.join(self.root_dir, f'{video_number:06}')\n        frames = []\n        # print(f\"Looking for frames in: {frame_dir}\")\n        for frame_path in sorted(glob.glob(os.path.join(frame_dir, '*.png'))):\n            frame = read_image(frame_path)\n            frame = transforms.ToPILImage()(frame)\n\n            if self.transform:\n                frame = self.transform(frame)\n            frames.append(frame)\n        # print(f\"Found {len(frames)} frames.\")\n\n        if not frames: \n            raise ValueError(f\"No frames found for video {video_number} in directory {frame_dir}\")\n        frames = torch.stack(frames)\n        return frames, label\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  \n    transforms.RandomHorizontalFlip(),  \n    transforms.RandomRotation(10), \n    transforms.ToTensor(),  \n])\n\ndataset = VideoDataset(annotations_df=annotations, root_dir='/kaggle/working/frames', transform=transform)","metadata":{"execution":{"iopub.status.busy":"2025-05-20T11:19:07.994755Z","iopub.execute_input":"2025-05-20T11:19:07.995660Z","iopub.status.idle":"2025-05-20T11:19:13.597742Z","shell.execute_reply.started":"2025-05-20T11:19:07.995627Z","shell.execute_reply":"2025-05-20T11:19:13.596927Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom collections import Counter\n\nnp.random.seed(21)\ndataset_size = len(dataset)\nindices = list(range(dataset_size))\nnp.random.shuffle(indices)\n\ntrain_split = int(np.floor(0.7 * dataset_size))\nval_split = int(np.floor(0.2 * dataset_size))\ntest_split = dataset_size - train_split - val_split\n\ntrain_indices = indices[:train_split]\nval_indices = indices[train_split:train_split + val_split]\ntest_indices = indices[train_split + val_split:]\n\nprint(f\"Total dataset size: {dataset_size}\")\nprint(f\"Training set size: {len(train_indices)}\")\nprint(f\"Validation set size: {len(val_indices)}\")\nprint(f\"Test set size: {len(test_indices)}\")\n\ntrain_sampler = SubsetRandomSampler(train_indices)\nval_sampler = SubsetRandomSampler(val_indices)\ntest_sampler = SubsetRandomSampler(test_indices)\n\ntrain_loader = DataLoader(dataset, batch_size=2, sampler=train_sampler)\nval_loader = DataLoader(dataset, batch_size=2, sampler=val_sampler)\ntest_loader = DataLoader(dataset, batch_size=2, sampler=test_sampler)\n\nprint(\"Total effective samples in training set:\", len(train_loader) * train_loader.batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:19:16.959304Z","iopub.execute_input":"2025-05-20T11:19:16.959802Z","iopub.status.idle":"2025-05-20T11:19:16.968982Z","shell.execute_reply.started":"2025-05-20T11:19:16.959756Z","shell.execute_reply":"2025-05-20T11:19:16.967838Z"}},"outputs":[{"name":"stdout","text":"Total dataset size: 1415\nTraining set size: 990\nValidation set size: 283\nTest set size: 142\nTotal effective samples in training set: 990\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.models import mobilenet_v3_small\nfrom transformers import TimesformerModel, TimesformerConfig\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport os\n\nclass VideoClassifier(nn.Module):\n    def __init__(self, num_classes, num_frames, hidden_size=512, num_layers=1, bidirectional=True, num_heads=4):\n        super().__init__()\n        self.config = TimesformerConfig(num_frames=num_frames, num_classes=num_classes)\n        self.timesformer = TimesformerModel(self.config)\n        self.lstm = nn.LSTM(input_size=self.config.hidden_size, hidden_size=hidden_size,\n                            num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n        self.multihead_attention = nn.MultiheadAttention(embed_dim=lstm_output_size, num_heads=num_heads, batch_first=True)\n        self.classifier = nn.Linear(lstm_output_size, num_classes)\n\n    def forward(self, x):\n        transformer_output = self.timesformer(x).last_hidden_state\n        lstm_output, _ = self.lstm(transformer_output)\n        attn_output, _ = self.multihead_attention(lstm_output, lstm_output, lstm_output)\n        context_vector = torch.sum(attn_output, dim=1)\n        logits = self.classifier(context_vector)\n        return logits\n\nclass DistilledCrashNet(nn.Module):\n    def __init__(self, num_classes=4, num_frames=20):\n        super().__init__()\n        self.mobilenet = mobilenet_v3_small(pretrained=True)\n        self.mobilenet.classifier = nn.Identity()\n        self.temporal_pool = nn.AdaptiveAvgPool1d(32)\n        self.lstm = nn.LSTM(input_size=576, hidden_size=128, num_layers=1, bidirectional=True)\n        self.attention = nn.MultiheadAttention(embed_dim=256, num_heads=4)\n        self.classifier = nn.Sequential(nn.Linear(256, 128), nn.ReLU(), nn.Linear(128, num_classes))\n\n    def forward(self, x):\n        batch_size, timesteps = x.shape[0], x.shape[1]\n        spatial_features = []\n        for t in range(timesteps):\n            frame_features = self.mobilenet(x[:, t])\n            spatial_features.append(frame_features)\n        temporal_features = torch.stack(spatial_features, dim=1)\n        temporal_features = self.temporal_pool(temporal_features.permute(0, 2, 1)).permute(0, 2, 1)\n        lstm_out, _ = self.lstm(temporal_features)\n        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n        context = torch.mean(attn_out, dim=1)\n        return self.classifier(context)\n\n# ------------------------------\n# 3. Distillation Trainer\n# ------------------------------\nclass DistillationTrainer:\n    def __init__(self, teacher, student, temp=3.0, alpha=0.7):\n        self.teacher = teacher.eval()\n        self.student = student.train()\n        self.temp = temp\n        self.alpha = alpha\n        self.kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n        self.ce_loss = nn.CrossEntropyLoss()\n        \n    def compute_loss(self, student_logits, teacher_logits, labels):\n        soft_loss = self.kl_loss(torch.log_softmax(student_logits / self.temp, dim=1),\n                                 torch.softmax(teacher_logits / self.temp, dim=1)) * (self.temp ** 2)\n        hard_loss = self.ce_loss(student_logits, labels)\n        return self.alpha * soft_loss + (1 - self.alpha) * hard_loss\n\n    def train_step(self, inputs, labels):\n        with torch.no_grad():\n            teacher_logits = self.teacher(inputs)\n        student_logits = self.student(inputs)\n        loss = self.compute_loss(student_logits, teacher_logits, labels)\n        return loss\n\n# ------------------------------\n# 4. Load Teacher Model\n# ------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nteacher = VideoClassifier(num_classes=4, num_frames=20).to(device)\nteacher.load_state_dict(torch.load(\"/kaggle/input/timesformer_mha_bilstm/pytorch/default/1/best_model.pth\", map_location=device))\n\n# ------------------------------\n# 5. Initialize Student & Distiller\n# ------------------------------\nstudent = DistilledCrashNet(num_classes=4, num_frames=20).to(device)\ndistiller = DistillationTrainer(teacher, student)\noptimizer = torch.optim.AdamW(student.parameters(), lr=3e-4, weight_decay=1e-5)\n\n# ------------------------------\n# 6. Distillation Training Loop\n# ------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:19:29.713020Z","iopub.execute_input":"2025-05-20T11:19:29.713337Z","iopub.status.idle":"2025-05-20T11:19:37.195401Z","shell.execute_reply.started":"2025-05-20T11:19:29.713312Z","shell.execute_reply":"2025-05-20T11:19:37.194371Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n100%|██████████| 9.83M/9.83M [00:00<00:00, 90.3MB/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ------------------------------\n# 6. Distillation Training Loop with Validation\n# ------------------------------\n\nbest_val_loss = float('inf')\npatience = 3\npatience_counter = 0\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # --- Training Phase ---\n    student.train()\n    total_loss = 0.0\n    for batch_idx, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        loss = distiller.train_step(inputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        if batch_idx % 100 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}], Loss: {loss.item():.4f}\")\n\n    avg_train_loss = total_loss / len(train_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Avg Train Loss: {avg_train_loss:.4f}\")\n\n    # --- Validation Phase ---\n    student.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            teacher_logits = teacher(inputs)\n            student_logits = student(inputs)\n            loss = distiller.compute_loss(student_logits, teacher_logits, labels)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    print(f\"Validation Loss after Epoch {epoch+1}: {avg_val_loss:.4f}\")\n\n    # --- Check for improvement ---\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(student.state_dict(), \"/kaggle/working/best_student_model.pth\")\n        print(\"Best student model saved!\")\n    else:\n        patience_counter += 1\n        print(f\"No improvement. Patience Counter: {patience_counter}/{patience}\")\n        if patience_counter >= patience:\n            print(\"Early stopping triggered.\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T11:19:41.769801Z","iopub.execute_input":"2025-05-20T11:19:41.770357Z","iopub.status.idle":"2025-05-20T13:37:49.896644Z","shell.execute_reply.started":"2025-05-20T11:19:41.770325Z","shell.execute_reply":"2025-05-20T13:37:49.895589Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Step [0], Loss: 3.8233\nEpoch [1/10], Step [100], Loss: 1.8438\nEpoch [1/10], Step [200], Loss: 0.1233\nEpoch [1/10], Step [300], Loss: 0.2688\nEpoch [1/10], Step [400], Loss: 0.2184\nEpoch [1/10] completed. Avg Train Loss: 0.8343\nValidation Loss after Epoch 1: 0.7920\nBest student model saved!\nEpoch [2/10], Step [0], Loss: 0.2707\nEpoch [2/10], Step [100], Loss: 1.0479\nEpoch [2/10], Step [200], Loss: 1.0045\nEpoch [2/10], Step [300], Loss: 0.3561\nEpoch [2/10], Step [400], Loss: 0.3118\nEpoch [2/10] completed. Avg Train Loss: 0.7633\nValidation Loss after Epoch 2: 0.7941\nNo improvement. Patience Counter: 1/3\nEpoch [3/10], Step [0], Loss: 1.0825\nEpoch [3/10], Step [100], Loss: 1.0703\nEpoch [3/10], Step [200], Loss: 1.0207\nEpoch [3/10], Step [300], Loss: 1.0453\nEpoch [3/10], Step [400], Loss: 0.4552\nEpoch [3/10] completed. Avg Train Loss: 0.7553\nValidation Loss after Epoch 3: 0.8092\nNo improvement. Patience Counter: 2/3\nEpoch [4/10], Step [0], Loss: 1.1088\nEpoch [4/10], Step [100], Loss: 0.2599\nEpoch [4/10], Step [200], Loss: 0.1678\nEpoch [4/10], Step [300], Loss: 0.1573\nEpoch [4/10], Step [400], Loss: 0.1299\nEpoch [4/10] completed. Avg Train Loss: 0.7585\nValidation Loss after Epoch 4: 0.7778\nBest student model saved!\nEpoch [5/10], Step [0], Loss: 2.3310\nEpoch [5/10], Step [100], Loss: 0.3524\nEpoch [5/10], Step [200], Loss: 1.0346\nEpoch [5/10], Step [300], Loss: 0.2014\nEpoch [5/10], Step [400], Loss: 0.2523\nEpoch [5/10] completed. Avg Train Loss: 0.7530\nValidation Loss after Epoch 5: 0.8109\nNo improvement. Patience Counter: 1/3\nEpoch [6/10], Step [0], Loss: 0.1856\nEpoch [6/10], Step [100], Loss: 1.1012\nEpoch [6/10], Step [200], Loss: 1.0420\nEpoch [6/10], Step [300], Loss: 0.2252\nEpoch [6/10], Step [400], Loss: 0.2642\nEpoch [6/10] completed. Avg Train Loss: 0.7503\nValidation Loss after Epoch 6: 0.8316\nNo improvement. Patience Counter: 2/3\nEpoch [7/10], Step [0], Loss: 0.1914\nEpoch [7/10], Step [100], Loss: 0.2998\nEpoch [7/10], Step [200], Loss: 0.9767\nEpoch [7/10], Step [300], Loss: 1.0200\nEpoch [7/10], Step [400], Loss: 0.9975\nEpoch [7/10] completed. Avg Train Loss: 0.7472\nValidation Loss after Epoch 7: 0.8192\nNo improvement. Patience Counter: 3/3\nEarly stopping triggered.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ------------------------------\n# 7. Evaluate Student on Test Set\n# ------------------------------\nstudent.load_state_dict(torch.load(\"/kaggle/working/best_student_model.pth\"))\nstudent.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = student(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nstudent_acc = 100 * correct / total\nprint(f\"Student Test Accuracy: {student_acc:.3f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T13:47:22.857791Z","iopub.execute_input":"2025-05-20T13:47:22.858163Z","iopub.status.idle":"2025-05-20T13:48:10.696037Z","shell.execute_reply.started":"2025-05-20T13:47:22.858135Z","shell.execute_reply":"2025-05-20T13:48:10.694932Z"}},"outputs":[{"name":"stdout","text":"Student Test Accuracy: 57.042%\n","output_type":"stream"}],"execution_count":9}]}