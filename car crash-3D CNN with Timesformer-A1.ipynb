{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8829388,"sourceType":"datasetVersion","datasetId":5312326},{"sourceId":8831197,"sourceType":"datasetVersion","datasetId":5313720},{"sourceId":137044,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":115997,"modelId":139227}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# /kaggle/input/car-crash-dataset-ccd/CrashBest\n# /kaggle/input/car-crash-dataset-ccd/CrashBest/C_000001_01.jpg\n# /kaggle/input/crash-1500/Crash-1500/000001.mp4\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-16T13:50:37.766324Z","iopub.status.idle":"2024-07-16T13:50:37.767145Z","shell.execute_reply.started":"2024-07-16T13:50:37.766865Z","shell.execute_reply":"2024-07-16T13:50:37.766885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Define the file you want to delete\nfile_to_delete = 'best_model.pth'\n\n# Delete the file\nif os.path.exists(file_to_delete):\n    os.remove(file_to_delete)\n    print(f\"File '{file_to_delete}' deleted successfully.\")\nelse:\n    print(f\"File '{file_to_delete}' does not exist.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T11:59:27.767369Z","iopub.execute_input":"2024-10-25T11:59:27.767768Z","iopub.status.idle":"2024-10-25T11:59:27.802876Z","shell.execute_reply.started":"2024-10-25T11:59:27.767735Z","shell.execute_reply":"2024-10-25T11:59:27.801926Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"File 'best_model.pth' deleted successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install ffmpeg","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-10-11T13:49:23.831211Z","iopub.execute_input":"2024-10-11T13:49:23.831617Z","iopub.status.idle":"2024-10-11T13:49:33.064224Z","shell.execute_reply.started":"2024-10-11T13:49:23.831579Z","shell.execute_reply":"2024-10-11T13:49:33.063308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ncsv_file_path = '/kaggle/input/preprocessed-df/preprocessed_df (1).csv'\nannotations = pd.read_csv(csv_file_path)\n\nclass_counts = annotations['Severity of the Crash'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:04:15.341466Z","iopub.execute_input":"2024-10-25T16:04:15.342192Z","iopub.status.idle":"2024-10-25T16:04:15.811930Z","shell.execute_reply.started":"2024-10-25T16:04:15.342159Z","shell.execute_reply":"2024-10-25T16:04:15.810795Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Severity of the Crash\nModerate    622\nMinor       348\nSevere      244\nmoderate     70\nsevere       57\nminor        31\nfatal        25\nFatal        18\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"annotations['Severity of the Crash'] = annotations['Severity of the Crash'].str.lower()\nclass_counts = annotations['Severity of the Crash'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:04:19.261414Z","iopub.execute_input":"2024-10-25T16:04:19.262245Z","iopub.status.idle":"2024-10-25T16:04:19.271753Z","shell.execute_reply.started":"2024-10-25T16:04:19.262207Z","shell.execute_reply":"2024-10-25T16:04:19.270755Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Severity of the Crash\nmoderate    692\nminor       379\nsevere      301\nfatal        43\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nvideo_dir = '/kaggle/input/crash-1500/Crash-1500'\nframes_dir = '/kaggle/working/frames' \n\nfor idx, row in annotations.iterrows():\n    video_number = row['Video Number']\n    video_path = os.path.join(video_dir, f'{video_number:06}.mp4')\n    output_dir = os.path.join(frames_dir, f'{video_number:06}')\n    os.makedirs(output_dir, exist_ok=True)\n\n    ffmpeg_command = f'ffmpeg -ss 00:00:01 -i \"{video_path}\" -vf fps=5 \"{output_dir}/frame%04d.png\"'\n#     print(f\"Running FFmpeg command: {ffmpeg_command}\")\n    os.system(ffmpeg_command)\n    \n    extracted_frames = os.listdir(output_dir)\n    print(f\"Extracted {len(extracted_frames)} frames for video {video_number}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageFilter,ImageEnhance\nimport cv2\nimport numpy as np\n\ndef denoise_frame(frame):\n    frame = np.array(frame)\n    denoised_frame = cv2.medianBlur(frame, 5)\n    return Image.fromarray(denoised_frame)\n\ndef deblur_frame(frame):\n    return frame.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n\ndef enhance_contrast(frame):\n    frame = np.array(frame)\n    lab = cv2.cvtColor(frame, cv2.COLOR_RGB2LAB)\n    l, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    cl = clahe.apply(l)\n    limg = cv2.merge((cl, a, b))\n    enhanced_frame = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n    return Image.fromarray(enhanced_frame)\n\ndef sharpen_frame(frame, factor=2.0):\n    enhancer = ImageEnhance.Sharpness(frame)\n    return enhancer.enhance(factor)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:04:23.852254Z","iopub.execute_input":"2024-10-25T16:04:23.853135Z","iopub.status.idle":"2024-10-25T16:04:24.029742Z","shell.execute_reply.started":"2024-10-25T16:04:23.853101Z","shell.execute_reply":"2024-10-25T16:04:24.029026Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nframes_dir = '/kaggle/working/frames'\nvideo_dirs = next(os.walk(frames_dir))[1]\nnum_videos = len(video_dirs)\n\nprint(f\"Number of videos with extracted frames: {num_videos}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T15:23:32.886003Z","iopub.execute_input":"2024-10-25T15:23:32.886728Z","iopub.status.idle":"2024-10-25T15:23:32.893919Z","shell.execute_reply.started":"2024-10-25T15:23:32.886694Z","shell.execute_reply":"2024-10-25T15:23:32.892796Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Number of videos with extracted frames: 1415\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport glob\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nfrom PIL import Image\n\nclass VideoDataset(Dataset):\n    def __init__(self, annotations_df, root_dir, transform=None, num_frames=20):\n        self.annotations = annotations_df\n        self.annotations['Severity of the Crash'] = self.annotations['Severity of the Crash'].str.lower()\n        self.root_dir = root_dir\n        self.transform = transform\n        self.label_map = {'minor': 0, 'moderate': 1, 'severe': 2,'fatal': 3}\n        self.num_frames = num_frames\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        video_number = self.annotations.iloc[idx]['Video Number']\n        label = self.label_map[self.annotations.iloc[idx]['Severity of the Crash']]\n        frame_dir = os.path.join(self.root_dir, f'{video_number:06}')\n        frames = []\n        # print(f\"Looking for frames in: {frame_dir}\")\n        for frame_path in sorted(glob.glob(os.path.join(frame_dir, '*.png'))):\n            frame = read_image(frame_path)\n            frame = transforms.ToPILImage()(frame)\n            frame = denoise_frame(frame)\n            frame = sharpen_frame(frame)\n            frame = enhance_contrast(frame)\n            frame = deblur_frame(frame)\n            if self.transform:\n                frame = self.transform(frame)\n            frames.append(frame)\n        # print(f\"Found {len(frames)} frames.\")\n\n        if not frames: \n            raise ValueError(f\"No frames found for video {video_number} in directory {frame_dir}\")\n        frames = torch.stack(frames)\n        return frames, label\n\ntransform = transforms.Compose([\n    transforms.Resize((112, 112)),  \n    transforms.RandomHorizontalFlip(),  \n    transforms.RandomRotation(10), \n    transforms.ToTensor(),  \n])\n\ndataset = VideoDataset(annotations_df=annotations, root_dir='/kaggle/working/frames', transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:04:28.666355Z","iopub.execute_input":"2024-10-25T16:04:28.667185Z","iopub.status.idle":"2024-10-25T16:04:33.321239Z","shell.execute_reply.started":"2024-10-25T16:04:28.667153Z","shell.execute_reply":"2024-10-25T16:04:33.320408Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import TimesformerModel, TimesformerConfig\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:04:38.640840Z","iopub.execute_input":"2024-10-25T16:04:38.641347Z","iopub.status.idle":"2024-10-25T16:04:39.525168Z","shell.execute_reply.started":"2024-10-25T16:04:38.641317Z","shell.execute_reply":"2024-10-25T16:04:39.524177Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom transformers import TimesformerConfig, TimesformerModel\nimport torch.nn.functional as F\n\nclass Hybrid3DCNNTimeSformer(nn.Module):\n    def __init__(self, num_classes, num_frames, cnn_out_channels=128, hidden_size=512, num_layers=1, bidirectional=True, num_heads=4):\n        super(Hybrid3DCNNTimeSformer, self).__init__()\n\n        # 3D CNN Layers\n        self.conv1 = nn.Conv3d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))  # Pooling only spatial dimensions\n        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv3d(64, cnn_out_channels, kernel_size=3, stride=1, padding=1)\n        \n        # TimeSformer configuration\n        self.timesformer_config = TimesformerConfig(num_classes=num_classes)\n        self.timesformer = TimesformerModel(self.timesformer_config)\n        \n        # Calculate output size for LSTM\n        self.dummy_input = torch.zeros(1, 3, num_frames, 112, 112)\n        self.flattened_size = self._get_conv_output_size(self.dummy_input)\n        \n        # Additional LSTM and Attention layers\n        self.lstm = nn.LSTM(input_size=self.flattened_size, hidden_size=hidden_size, num_layers=num_layers,\n                            bidirectional=bidirectional, batch_first=True)\n        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n        self.multihead_attention = nn.MultiheadAttention(embed_dim=lstm_output_size, num_heads=num_heads, batch_first=True)\n        \n        # Classifier layer\n        self.classifier = nn.Linear(lstm_output_size, num_classes)\n\n    def _get_conv_output_size(self, input_tensor):\n        x = self.conv1(input_tensor)\n        x = self.pool(F.relu(x))\n        x = self.conv2(x)\n        x = self.pool(F.relu(x))\n        x = self.conv3(x)\n        x = self.pool(F.relu(x))\n        print(\"Shape after final CNN layer:\", x.shape)  \n        return x.numel()\n    \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n\n        print(\"Shape before reshaping for TimeSformer:\", x.shape)\n        \n        # Reshape to (batch_size, num_frames, num_channels, height, width)\n        cnn_features = x.view(x.size(0), 20, 128, 14, 14)  \n        print(\"Shape of cnn_features after reshaping:\", cnn_features.shape)\n\n        # TimeSformer processing\n        transformer_output = self.timesformer(cnn_features).last_hidden_state\n\n        # LSTM processing\n        lstm_output, _ = self.lstm(transformer_output)\n        attn_output, _ = self.multihead_attention(lstm_output, lstm_output, lstm_output)\n \n        # Sum over attention outputs\n        context_vector = torch.sum(attn_output, dim=1)\n\n        logits = self.classifier(context_vector)\n        return logits\n\n# Model instantiation\nmodel = Hybrid3DCNNTimeSformer(num_classes=4, num_frames=20)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T16:04:43.926916Z","iopub.execute_input":"2024-10-25T16:04:43.927564Z","iopub.status.idle":"2024-10-25T16:05:09.375075Z","shell.execute_reply.started":"2024-10-25T16:04:43.927526Z","shell.execute_reply":"2024-10-25T16:05:09.373682Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Shape after final CNN layer: torch.Size([1, 128, 20, 14, 14])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m model \u001b[38;5;241m=\u001b[39m Hybrid3DCNNTimeSformer(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     70\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    213\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_apply(fn, recurse)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Resets _flat_weights\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_flat_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:155\u001b[0m, in \u001b[0;36mRNNBase._init_flat_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, wn) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, wn) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    152\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m wn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights_names]\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weight_refs \u001b[38;5;241m=\u001b[39m [weakref\u001b[38;5;241m.\u001b[39mref(w) \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m                           \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights]\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:206\u001b[0m, in \u001b[0;36mRNNBase.flatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    205\u001b[0m     num_weights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 206\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cudnn_rnn_flatten_weight\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cudnn_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 14.74 GiB of which 6.46 GiB is free. Process 6118 has 8.28 GiB memory in use. Of the allocated memory 8.12 GiB is allocated by PyTorch, and 51.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 14.74 GiB of which 6.46 GiB is free. Process 6118 has 8.28 GiB memory in use. Of the allocated memory 8.12 GiB is allocated by PyTorch, and 51.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, WeightedRandomSampler, SubsetRandomSampler\nfrom collections import Counter\n\nnp.random.seed(21)\ndataset_size = len(dataset)\nindices = list(range(dataset_size))\nnp.random.shuffle(indices)\n\ntrain_split = int(np.floor(0.7 * dataset_size))\nval_split = int(np.floor(0.2 * dataset_size))\ntest_split = dataset_size - train_split - val_split\n\ntrain_indices = indices[:train_split]\nval_indices = indices[train_split:train_split + val_split]\ntest_indices = indices[train_split + val_split:]\n\nprint(f\"Total dataset size: {dataset_size}\")\nprint(f\"Training set size: {len(train_indices)}\")\nprint(f\"Validation set size: {len(val_indices)}\")\nprint(f\"Test set size: {len(test_indices)}\")\n\n# train_labels = [annotations['Severity of the Crash'][idx] for idx in train_indices]\n# class_counts = Counter(train_labels)\n# total_samples = len(train_labels)\n# class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n# print(class_weights)\n# sample_weights = [class_weights[annotations['Severity of the Crash'][idx]] for idx in train_indices]\n# sample_weights = torch.tensor(sample_weights, dtype=torch.float)\n\n# train_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\ntrain_sampler = SubsetRandomSampler(train_indices)\nval_sampler = SubsetRandomSampler(val_indices)\ntest_sampler = SubsetRandomSampler(test_indices)\n\ntrain_loader = DataLoader(dataset, batch_size=2, sampler=train_sampler)\nval_loader = DataLoader(dataset, batch_size=2, sampler=val_sampler)\ntest_loader = DataLoader(dataset, batch_size=2, sampler=test_sampler)\n\nprint(\"Total effective samples in training set:\", len(train_loader) * train_loader.batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T15:24:23.117682Z","iopub.execute_input":"2024-10-25T15:24:23.118478Z","iopub.status.idle":"2024-10-25T15:24:23.129311Z","shell.execute_reply.started":"2024-10-25T15:24:23.118441Z","shell.execute_reply":"2024-10-25T15:24:23.128207Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Total dataset size: 1415\nTraining set size: 990\nValidation set size: 283\nTest set size: 142\nTotal effective samples in training set: 990\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nnum_epochs = 3\nbest_val_acc = 0.0  \nsave_path = '/kaggle/working/best_model.pth'  \n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for data in train_loader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        inputs = inputs.permute(0, 2, 1, 3, 4)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    # Here the model is evaluated on the validation set\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in val_loader:\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = 100 * correct / total\n\n    print('[%d] loss: %.3f, val_acc: %.3f' % (epoch + 1, running_loss / len(train_loader), val_acc))\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), save_path)\n        print(f\"New best model saved with val_acc: {best_val_acc:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T15:47:33.033967Z","iopub.execute_input":"2024-10-25T15:47:33.034467Z","iopub.status.idle":"2024-10-25T15:47:37.733048Z","shell.execute_reply.started":"2024-10-25T15:47:33.034433Z","shell.execute_reply":"2024-10-25T15:47:37.731746Z"},"trusted":true},"execution_count":26,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[25], line 52\u001b[0m, in \u001b[0;36mHybrid3DCNNTimeSformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#         print(\"Shape before reshaping for TimeSformer:\", x.shape)\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m         cnn_features \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of cnn_features after reshaping:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cnn_features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;66;03m# TimeSformer processing\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 20, 128, 14, 14]' is invalid for input of size 100352"],"ename":"RuntimeError","evalue":"shape '[2, 20, 128, 14, 14]' is invalid for input of size 100352","output_type":"error"}]},{"cell_type":"markdown","source":"TEST ACCURACY, RECALL PART","metadata":{}},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef calculate_metrics(model, data_loader, device):\n    model.eval()\n    correct_predictions = 0\n    custom_correct = 0\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for data in data_loader:\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            # The probabilities from the outputs\n            probabilities = torch.softmax(outputs, dim=1)\n            top_probs, top_classes = torch.topk(probabilities, 2, dim=1)\n\n            p1 = top_classes[:, 0]  \n            p2 = top_classes[:, 1]  \n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(p1.cpu().numpy())\n\n            for i in range(len(labels)):\n                true_label = labels[i].item()\n                predicted1 = p1[i].item()\n                predicted2 = p2[i].item()\n                \n                if abs(predicted1-predicted2) == 1:\n                    if(predicted1==true_label or predicted2==true_label):  \n                        custom_correct += 1 \n                else:\n                    if predicted1 == true_label:\n                        custom_correct += 1\n\n    total_samples = len(all_labels)\n    custom_accuracy = custom_correct / total_samples\n    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n\n    print(f\"Custom Accuracy: {custom_accuracy * 100:.2f}%\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"Recall: {recall:.2f}\")\n    print(f\"F1 Score: {f1:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T16:44:14.183545Z","iopub.execute_input":"2024-10-21T16:44:14.184319Z","iopub.status.idle":"2024-10-21T16:44:14.910062Z","shell.execute_reply.started":"2024-10-21T16:44:14.184284Z","shell.execute_reply":"2024-10-21T16:44:14.909173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(save_path))\ncalculate_metrics(model, test_loader, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T16:44:20.288330Z","iopub.execute_input":"2024-10-21T16:44:20.288675Z","iopub.status.idle":"2024-10-21T16:51:09.810342Z","shell.execute_reply.started":"2024-10-21T16:44:20.288650Z","shell.execute_reply":"2024-10-21T16:51:09.809258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PARTS WHERE MY CODE GAVE AN ERROR","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ThreeDCNN(nn.Module):\n    def __init__(self, out_features=512):\n        super(ThreeDCNN, self).__init__()\n        self.conv1 = nn.Conv3d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=2)\n        self.pool = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, stride=2, padding=2)\n        self.conv3 = nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=2)\n        self.fc = nn.Linear(512, out_features)  \n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = F.relu(self.conv3(x))\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:28:25.795892Z","iopub.execute_input":"2024-10-25T14:28:25.796448Z","iopub.status.idle":"2024-10-25T14:28:25.806082Z","shell.execute_reply.started":"2024-10-25T14:28:25.796415Z","shell.execute_reply":"2024-10-25T14:28:25.804953Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class VideoClassifier(nn.Module):\n    def __init__(self, num_classes=4, num_frames=20, cnn_output_dim=512):\n        super(VideoClassifier, self).__init__()\n        self.cnn = ThreeDCNN(out_features=cnn_output_dim)  # 3D CNN feature extractor\n        self.timesformer_config = TimesformerConfig(\n            num_frames=num_frames,\n            hidden_size=cnn_output_dim,\n            num_classes=num_classes\n        )\n        self.timesformer = TimesformerModel(self.timesformer_config)\n        self.fc = nn.Linear(cnn_output_dim, num_classes)\n\n    def forward(self, x):\n        batch_size, channels, depth, height, width = x.size()\n        cnn_features = []\n        for i in range(depth):  \n            frame = x[:, :, i, :, :].unsqueeze(2)  \n            cnn_features.append(self.cnn(frame))\n        cnn_features = torch.stack(cnn_features, dim=1)  \n\n        cnn_features = cnn_features.permute(0, 2, 1, 3, 4)  \n        timesformer_output = self.timesformer(cnn_features).last_hidden_state\n        logits = self.fc(torch.mean(timesformer_output, dim=1)) \n\n        return logits\n\nmodel = VideoClassifier(num_classes=4, num_frames=20)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:29:15.086712Z","iopub.execute_input":"2024-10-25T14:29:15.087177Z","iopub.status.idle":"2024-10-25T14:29:16.819439Z","shell.execute_reply.started":"2024-10-25T14:29:15.087136Z","shell.execute_reply":"2024-10-25T14:29:16.818515Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"VideoClassifier(\n  (cnn): ThreeDCNN(\n    (conv1): Conv3d(3, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(2, 2, 2))\n    (pool): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (conv2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(2, 2, 2))\n    (conv3): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(2, 2, 2))\n    (fc): Linear(in_features=512, out_features=512, bias=True)\n  )\n  (timesformer): TimesformerModel(\n    (embeddings): TimesformerEmbeddings(\n      (patch_embeddings): TimesformerPatchEmbeddings(\n        (projection): Conv2d(3, 512, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (pos_drop): Dropout(p=0.0, inplace=False)\n      (time_drop): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): TimesformerEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x TimesformerLayer(\n          (drop_path): Identity()\n          (attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): TimesformerIntermediate(\n            (dense): Linear(in_features=512, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): TimesformerOutput(\n            (dense): Linear(in_features=3072, out_features=512, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n          (temporal_layernorm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n          (temporal_attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (temporal_dense): Linear(in_features=512, out_features=512, bias=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n  )\n  (fc): Linear(in_features=512, out_features=4, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nnum_epochs = 3\nbest_val_acc = 0.0  \nsave_path = '/kaggle/working/best_model.pth'  \n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for data in train_loader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        inputs = inputs.permute(0, 2, 1, 3, 4)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    # Here the model is evaluated on the validation set\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in val_loader:\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = 100 * correct / total\n\n    print('[%d] loss: %.3f, val_acc: %.3f' % (epoch + 1, running_loss / len(train_loader), val_acc))\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), save_path)\n        print(f\"New best model saved with val_acc: {best_val_acc:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:29:29.606424Z","iopub.execute_input":"2024-10-25T14:29:29.607048Z","iopub.status.idle":"2024-10-25T14:29:35.739306Z","shell.execute_reply.started":"2024-10-25T14:29:29.607018Z","shell.execute_reply":"2024-10-25T14:29:35.738000Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mVideoClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m     cnn_features\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn(frame))\n\u001b[1;32m     19\u001b[0m cnn_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(cnn_features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \n\u001b[0;32m---> 21\u001b[0m cnn_features \u001b[38;5;241m=\u001b[39m \u001b[43mcnn_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     22\u001b[0m timesformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesformer(cnn_features)\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     23\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(torch\u001b[38;5;241m.\u001b[39mmean(timesformer_output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)) \n","\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 5"],"ename":"RuntimeError","evalue":"permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 5","output_type":"error"}]}]}