{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8829388,"sourceType":"datasetVersion","datasetId":5312326},{"sourceId":8831197,"sourceType":"datasetVersion","datasetId":5313720}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# /kaggle/input/car-crash-dataset-ccd/CrashBest\n# /kaggle/input/car-crash-dataset-ccd/CrashBest/C_000001_01.jpg\n# /kaggle/input/crash-1500/Crash-1500/000001.mp4\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-16T13:50:37.766324Z","iopub.status.idle":"2024-07-16T13:50:37.767145Z","shell.execute_reply.started":"2024-07-16T13:50:37.766865Z","shell.execute_reply":"2024-07-16T13:50:37.766885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install ffmpeg","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-07T07:46:20.510927Z","iopub.execute_input":"2024-09-07T07:46:20.512325Z","iopub.status.idle":"2024-09-07T07:46:29.104375Z","shell.execute_reply.started":"2024-09-07T07:46:20.512257Z","shell.execute_reply":"2024-09-07T07:46:29.103154Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease\nGet:2 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]\nGet:3 https://packages.cloud.google.com/apt gcsfuse-focal InRelease [1227 B]\nGet:4 https://packages.cloud.google.com/apt cloud-sdk InRelease [1618 B]       \nGet:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]        \nGet:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\nGet:7 https://packages.cloud.google.com/apt google-fast-socket InRelease [1071 B]\nHit:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease               \nGet:9 https://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [26.3 kB]\nGet:10 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [3232 kB]\nGet:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1673 kB]\nGet:12 https://packages.cloud.google.com/apt cloud-sdk/main all Packages [1517 kB]\nGet:13 https://packages.cloud.google.com/apt google-fast-socket/main amd64 Packages [15.4 kB]\nGet:14 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [33.5 kB]\nGet:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1555 kB]\nGet:16 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1270 kB]\nGet:17 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [4079 kB]\nGet:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4399 kB]\nGet:19 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [30.9 kB]\nGet:20 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3937 kB]\nGet:21 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3928 kB]\nFetched 26.0 MB in 2s (11.9 MB/s)                           \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n0 upgraded, 0 newly installed, 0 to remove and 100 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ncsv_file_path = '/kaggle/input/preprocessed-df/preprocessed_df (1).csv'\nannotations = pd.read_csv(csv_file_path)\n\nclass_counts = annotations['Severity of the Crash'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:46:33.340472Z","iopub.execute_input":"2024-09-07T07:46:33.340882Z","iopub.status.idle":"2024-09-07T07:46:33.843584Z","shell.execute_reply.started":"2024-09-07T07:46:33.340842Z","shell.execute_reply":"2024-09-07T07:46:33.842242Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Severity of the Crash\nModerate    622\nMinor       348\nSevere      244\nmoderate     70\nsevere       57\nminor        31\nfatal        25\nFatal        18\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"annotations['Severity of the Crash'] = annotations['Severity of the Crash'].str.lower()\nclass_counts = annotations['Severity of the Crash'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:46:37.251703Z","iopub.execute_input":"2024-09-07T07:46:37.252061Z","iopub.status.idle":"2024-09-07T07:46:37.260786Z","shell.execute_reply.started":"2024-09-07T07:46:37.252031Z","shell.execute_reply":"2024-09-07T07:46:37.259517Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Severity of the Crash\nmoderate    692\nminor       379\nsevere      301\nfatal        43\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"annotations['Severity of the Crash'] = annotations['Severity of the Crash'].str.replace('fatal', 'severe', case=False)\nannotations['label'] = annotations['label'].replace(3, 2)\nclass_counts = annotations['Severity of the Crash'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:46:41.025833Z","iopub.execute_input":"2024-09-07T07:46:41.026210Z","iopub.status.idle":"2024-09-07T07:46:41.037359Z","shell.execute_reply.started":"2024-09-07T07:46:41.026180Z","shell.execute_reply":"2024-09-07T07:46:41.036281Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Severity of the Crash\nmoderate    692\nminor       379\nsevere      344\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nvideo_dir = '/kaggle/input/crash-1500/Crash-1500'\nframes_dir = '/kaggle/working/frames' \n\nfor idx, row in annotations.iterrows():\n    video_number = row['Video Number']\n    video_path = os.path.join(video_dir, f'{video_number:06}.mp4')\n    output_dir = os.path.join(frames_dir, f'{video_number:06}')\n    os.makedirs(output_dir, exist_ok=True)\n\n    ffmpeg_command = f'ffmpeg -ss 00:00:01 -i \"{video_path}\" -vf fps=5 \"{output_dir}/frame%04d.png\"'\n#     print(f\"Running FFmpeg command: {ffmpeg_command}\")\n    os.system(ffmpeg_command)\n    \n    extracted_frames = os.listdir(output_dir)\n    print(f\"Extracted {len(extracted_frames)} frames for video {video_number}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageFilter,ImageEnhance\nimport cv2\nimport numpy as np\n\ndef denoise_frame(frame):\n    frame = np.array(frame)\n    denoised_frame = cv2.medianBlur(frame, 5)\n    return Image.fromarray(denoised_frame)\n\ndef deblur_frame(frame):\n    return frame.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n\ndef enhance_contrast(frame):\n    frame = np.array(frame)\n    lab = cv2.cvtColor(frame, cv2.COLOR_RGB2LAB)\n    l, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    cl = clahe.apply(l)\n    limg = cv2.merge((cl, a, b))\n    enhanced_frame = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n    return Image.fromarray(enhanced_frame)\n\ndef sharpen_frame(frame, factor=2.0):\n    enhancer = ImageEnhance.Sharpness(frame)\n    return enhancer.enhance(factor)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T13:24:14.576562Z","iopub.execute_input":"2024-08-09T13:24:14.577225Z","iopub.status.idle":"2024-08-09T13:24:15.061301Z","shell.execute_reply.started":"2024-08-09T13:24:14.577191Z","shell.execute_reply":"2024-08-09T13:24:15.060369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nframes_dir = '/kaggle/working/frames'\nvideo_dirs = next(os.walk(frames_dir))[1]\nnum_videos = len(video_dirs)\n\nprint(f\"Number of videos with extracted frames: {num_videos}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:58:43.260017Z","iopub.execute_input":"2024-09-07T07:58:43.260417Z","iopub.status.idle":"2024-09-07T07:58:43.268464Z","shell.execute_reply.started":"2024-09-07T07:58:43.260385Z","shell.execute_reply":"2024-09-07T07:58:43.267434Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Number of videos with extracted frames: 1415\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install git+https://github.com/huggingface/transformers.git","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:58:46.665638Z","iopub.execute_input":"2024-09-07T07:58:46.666001Z","iopub.status.idle":"2024-09-07T07:59:42.836952Z","shell.execute_reply.started":"2024-09-07T07:58:46.665971Z","shell.execute_reply":"2024-09-07T07:59:42.835785Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers.git\n  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-oa7tpifz\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-oa7tpifz\n  Resolved https://github.com/huggingface/transformers.git to commit 66bc4def9505fa7c7fe4aa7a248c34a026bb552b\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.45.0.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (2024.2.2)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9680479 sha256=0efc7bba02a756c92b8b25c077daae6be60a360d466c8d83cd1b3506466ca1b9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ibbzx0y5/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\nSuccessfully installed transformers-4.45.0.dev0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoConfig, CLIPVisionConfig, LlamaConfig\nfrom transformers import LlavaNextVideoProcessor\n# Initializing a CLIP-vision config\nvision_config = CLIPVisionConfig()\n\n# Initializing a Llama config\ntext_config = LlamaConfig()\n\nconfiguration = LlavaNextVideoConfig(vision_config, text_config)\n\nmodel = LlavaNextVideoForConditionalGeneration(configuration)\n\nprocessor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")","metadata":{"execution":{"iopub.status.busy":"2024-09-07T07:59:50.509964Z","iopub.execute_input":"2024-09-07T07:59:50.510371Z","iopub.status.idle":"2024-09-07T08:02:22.749606Z","shell.execute_reply.started":"2024-09-07T07:59:50.510332Z","shell.execute_reply":"2024-09-07T08:02:22.748639Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-09-07 07:59:58.214908: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-07 07:59:58.215026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-07 07:59:58.341863: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/741 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da7e495c6e84406fbb0d9b11d4800d3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc1018c9c9c34180b72cd1c91ecabbb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12ca11789af048508f39d7fed549e39f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c41115d0244df99da7355343df3228"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61934bb5c2c14bb7967b049182d1f534"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b96be74c8a849f4a9701a07e2505bf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/838 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7295e34a473841b48b92de56e6368df9"}},"metadata":{}}]},{"cell_type":"code","source":"import transformers\n\nprint(transformers.__all__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-09-07T08:02:27.939697Z","iopub.execute_input":"2024-09-07T08:02:27.940449Z","iopub.status.idle":"2024-09-07T08:02:27.953262Z","shell.execute_reply.started":"2024-09-07T08:02:27.940413Z","shell.execute_reply":"2024-09-07T08:02:27.952272Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"LlavaNextVideoForConditionalGeneration(\n  (vision_tower): CLIPVisionModel(\n    (vision_model): CLIPVisionTransformer(\n      (embeddings): CLIPVisionEmbeddings(\n        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n        (position_embedding): Embedding(50, 768)\n      )\n      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (encoder): CLIPEncoder(\n        (layers): ModuleList(\n          (0-11): 12 x CLIPEncoderLayer(\n            (self_attn): CLIPSdpaAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (multi_modal_projector): LlavaNextVideoMultiModalProjector(\n    (linear_1): Linear(in_features=768, out_features=4096, bias=True)\n    (act): GELUActivation()\n    (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n  )\n  (language_model): LlamaForCausalLM(\n    (model): LlamaModel(\n      (embed_tokens): Embedding(32000, 4096)\n      (layers): ModuleList(\n        (0-31): 32 x LlamaDecoderLayer(\n          (self_attn): LlamaSdpaAttention(\n            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n            (rotary_emb): LlamaRotaryEmbedding()\n          )\n          (mlp): LlamaMLP(\n            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n        )\n      )\n      (norm): LlamaRMSNorm((4096,), eps=1e-06)\n      (rotary_emb): LlamaRotaryEmbedding()\n    )\n    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n  )\n  (vision_resampler): LlavaNextVideoPooler(\n    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nfrom PIL import Image\nimport glob\n\nclass VideoToTextDataset(Dataset):\n    def __init__(self, annotations_df, root_dir, transform=None, num_frames=20):\n        self.annotations = annotations_df\n        self.root_dir = root_dir\n        self.transform = transform\n        self.num_frames = num_frames\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        video_number = self.annotations.iloc[idx]['Video Number']\n        explanation = self.annotations.iloc[idx]['Explanation']\n        \n        frame_dir = os.path.join(self.root_dir, f'{video_number:06}')\n        frames = []\n        for frame_path in sorted(glob.glob(os.path.join(frame_dir, '*.png'))):\n            frame = Image.open(frame_path).convert(\"RGB\")\n            if self.transform:\n                frame = self.transform(frame)\n            frames.append(frame)\n\n        if len(frames) < self.num_frames:\n            raise ValueError(f\"Not enough frames found for video {video_number}\")\n\n        frames = torch.stack(frames[:self.num_frames])\n        \n        return frames, explanation\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\ndataset = VideoToTextDataset(annotations_df=annotations, root_dir='/kaggle/working/frames', transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T08:02:36.966955Z","iopub.execute_input":"2024-09-07T08:02:36.967504Z","iopub.status.idle":"2024-09-07T08:02:36.980236Z","shell.execute_reply.started":"2024-09-07T08:02:36.967460Z","shell.execute_reply":"2024-09-07T08:02:36.979281Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-09-07T08:02:59.681560Z","iopub.execute_input":"2024-09-07T08:02:59.681974Z","iopub.status.idle":"2024-09-07T08:02:59.687682Z","shell.execute_reply.started":"2024-09-07T08:02:59.681943Z","shell.execute_reply":"2024-09-07T08:02:59.686543Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import random_split, DataLoader\nnp.random.seed(21)\ndataset_size = len(dataset)\ntrain_size = int(0.7 * dataset_size)\nval_size = int(0.2 * dataset_size)\ntest_size = dataset_size - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T08:03:52.206300Z","iopub.execute_input":"2024-09-07T08:03:52.206727Z","iopub.status.idle":"2024-09-07T08:03:52.218486Z","shell.execute_reply.started":"2024-09-07T08:03:52.206694Z","shell.execute_reply":"2024-09-07T08:03:52.217260Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T08:04:31.295585Z","iopub.execute_input":"2024-09-07T08:04:31.296013Z","iopub.status.idle":"2024-09-07T08:04:31.465392Z","shell.execute_reply.started":"2024-09-07T08:04:31.295980Z","shell.execute_reply":"2024-09-07T08:04:31.463987Z"},"trusted":true},"execution_count":18,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2920\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2915\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2916\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2917\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2918\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2919\u001b[0m         )\n\u001b[0;32m-> 2920\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (3 times)]\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 14.74 GiB of which 10.12 MiB is free. Process 3322 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 47.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 14.74 GiB of which 10.12 MiB is free. Process 3322 has 14.73 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 47.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"from torch.optim import AdamW\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nnum_epochs = 2\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for data in train_loader:\n        frames, explanations = data\n        frames = frames.to(device)\n        \n        inputs = processor(videos=frames, text=explanations, return_tensors=\"pt\").to(device)\n        \n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(pixel_values=inputs[\"pixel_values\"], labels=inputs[\"input_ids\"])\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n\n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for data in val_loader:\n            frames, explanations = data\n            frames = frames.to(device)\n\n            inputs = processor(videos=frames, text=explanations, return_tensors=\"pt\").to(device)\n\n            outputs = model(pixel_values=inputs[\"pixel_values\"], labels=inputs[\"input_ids\"])\n            val_loss += outputs.loss.item()\n\n    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss / len(val_loader)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing phase\nmodel.eval()\ntest_loss = 0.0\nwith torch.no_grad():\n    for data in test_loader:\n        frames, explanations = data\n        frames = frames.to(device)\n        inputs = processor(videos=frames, text=explanations, return_tensors=\"pt\").to(device)\n\n        # Forward pass\n        outputs = model(pixel_values=inputs[\"pixel_values\"], labels=inputs[\"input_ids\"])\n        test_loss += outputs.loss.item()\n\nprint(f\"Test Loss: {test_loss / len(test_loader)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\ntoken = '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'\nlogin(token=token)","metadata":{"execution":{"iopub.status.busy":"2024-09-06T18:08:09.943504Z","iopub.execute_input":"2024-09-06T18:08:09.943912Z","iopub.status.idle":"2024-09-06T18:08:10.582371Z","shell.execute_reply.started":"2024-09-06T18:08:09.943876Z","shell.execute_reply":"2024-09-06T18:08:10.581104Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install GPUtil\n\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache()                           \n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T08:05:28.431498Z","iopub.execute_input":"2024-09-07T08:05:28.432432Z","iopub.status.idle":"2024-09-07T08:05:46.311547Z","shell.execute_reply.started":"2024-09-07T08:05:28.432395Z","shell.execute_reply":"2024-09-07T08:05:46.310319Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting GPUtil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: GPUtil\n  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7393 sha256=9f50956a7c0e64a24e02aad55555cd43c6f06bc6a890047cdf33fb9ba20b1b35\n  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\nSuccessfully built GPUtil\nInstalling collected packages: GPUtil\nSuccessfully installed GPUtil-1.4.0\nInitial GPU Usage\n| ID | GPU | MEM |\n------------------\n|  0 |  0% | 98% |\n|  1 |  0% |  0% |\nGPU Usage after emptying the cache\n| ID | GPU  | MEM |\n-------------------\n|  0 | 100% |  1% |\n|  1 |   0% |  0% |\n","output_type":"stream"}]}]}